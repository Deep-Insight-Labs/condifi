

# ConDiFi: Convergent and Divergent Thinking Benchmark Dataset

This repository contains the benchmark datasets for evaluating convergent and divergent thinking capabilities in large language models, as described in our paper accepted at the [KDD 2025 Workshop on Generative AI for Evaluation](https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/).

## Paper

**Reasoning Beyond the Obvious: Evaluating Divergent and Convergent Thinking in LLMs for Financial Scenarios**

*Zhuang Qiang Bok, Watson Wei Khong Chua*

ðŸ“„ [Read the paper on arXiv](https://arxiv.org/abs/2507.18368)

## Repository Structure

- `convergent_thinking_dataset/` - Dataset and evaluation framework for convergent thinking tasks
- `divergent_thinking_dataset/` - Dataset and evaluation framework for divergent thinking tasks  

Each dataset folder contains its own README with detailed information about the specific benchmark, evaluation metrics, and usage instructions.

## Citation

If you use this dataset in your research, please cite our paper:

```bibtex
@misc{bokandchua2025reasoningobvious,
      title={Reasoning Beyond the Obvious: Evaluating Divergent and Convergent Thinking in LLMs for Financial Scenarios}, 
      author={Zhuang Qiang Bok and Watson Wei Khong Chua},
      year={2025},
      eprint={2507.18368},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2507.18368}, 
}
```

## License

This work is licensed under Creative Commons.
